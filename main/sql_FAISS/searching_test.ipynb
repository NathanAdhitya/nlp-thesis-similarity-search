{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a38e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a697e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"\",\n",
    ")\n",
    "cursor = database.cursor()\n",
    "cursor.execute(\"USE nlp_thesis_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9571cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be139aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "\n",
    "def get_embedding(text, model):\n",
    "    # Handle None values or empty strings\n",
    "    if text is None or text == \"\":\n",
    "        return None\n",
    "    # Convert text to embedding using the provided model\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "class ThesisSimilaritySearch:\n",
    "    def __init__(self, model, use_title=False, use_abstract=True):\n",
    "        \"\"\"\n",
    "        Initialize the similarity search engine\n",
    "        \n",
    "        Args:\n",
    "            model: The SentenceTransformer model to use for encoding queries\n",
    "            use_title: Whether to use title embeddings for similarity search\n",
    "            use_abstract: Whether to use abstract embeddings for similarity search\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.use_title = use_title\n",
    "        self.use_abstract = use_abstract\n",
    "        \n",
    "        # Model name is used in column names\n",
    "        self.model_name = model._modules['0'].auto_model.config.name_or_path\n",
    "        \n",
    "        # These will be populated when load_index is called\n",
    "        self.index = None\n",
    "        self.paper_ids = []\n",
    "        self.paper_metadata = {}\n",
    "    \n",
    "    def load_index(self, cursor, limit=None):\n",
    "        \"\"\"\n",
    "        Load embeddings from database and build FAISS index\n",
    "        \n",
    "        Args:\n",
    "            cursor: MySQL database cursor\n",
    "            limit: Optional limit on number of papers to load\n",
    "        \"\"\"\n",
    "        # Determine which embedding column to use\n",
    "        if self.use_abstract and not self.use_title:\n",
    "            embedding_column = f\"abstract_embeddings_{self.model_name[22:]}\"  # Remove 'sentence-transformers/' prefix\n",
    "            print(f\"Using abstract embeddings ({embedding_column[22:]})\")\n",
    "        elif self.use_title and not self.use_abstract:\n",
    "            embedding_column = f\"title_embeddings_{self.model_name[22:]}\"  # Remove 'sentence-transformers/' prefix\n",
    "            print(f\"Using title embeddings ({embedding_column[22:]})\")\n",
    "        else:\n",
    "            # Default to abstract if both or neither are specified\n",
    "            embedding_column = f\"abstract_embeddings_{self.model_name[22:]}\"  # Remove 'sentence-transformers/' prefix\n",
    "            print(f\"Using abstract embeddings ({embedding_column[22:]})\")\n",
    "        \n",
    "        # Build query with limit if provided\n",
    "        query = f\"\"\"\n",
    "        SELECT id, title, abstract, `{embedding_column}`\n",
    "        FROM dewey_papers \n",
    "        WHERE `{embedding_column}` IS NOT NULL\n",
    "        \"\"\"\n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "            \n",
    "        # Execute query\n",
    "        cursor.execute(query)\n",
    "        papers = cursor.fetchall()\n",
    "        \n",
    "        if not papers:\n",
    "            raise ValueError(f\"No papers found with {embedding_column} not null\")\n",
    "            \n",
    "        print(f\"Loaded {len(papers)} papers with embeddings\")\n",
    "        \n",
    "        # Extract data for index building\n",
    "        self.paper_ids = []\n",
    "        self.paper_metadata = {}\n",
    "        embeddings = []\n",
    "        \n",
    "        for paper in papers:\n",
    "            paper_id = paper[0]\n",
    "            title = paper[1]\n",
    "            abstract = paper[2]\n",
    "            embedding_json = paper[3]\n",
    "            \n",
    "            # Skip papers with null embeddings\n",
    "            if embedding_json is None:\n",
    "                continue\n",
    "                \n",
    "            # Parse JSON embedding if it's a string\n",
    "            if isinstance(embedding_json, str):\n",
    "                try:\n",
    "                    embedding = json.loads(embedding_json)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding embedding for paper {paper_id}\")\n",
    "                    continue\n",
    "            else:\n",
    "                embedding = embedding_json\n",
    "                \n",
    "            # Add to our collections\n",
    "            self.paper_ids.append(paper_id)\n",
    "            self.paper_metadata[paper_id] = {\n",
    "                'id': paper_id,\n",
    "                'title': title,\n",
    "                'abstract': abstract,\n",
    "                'authors': [],     # Initialize empty authors list to be filled later\n",
    "                'contributors': []  # Initialize empty contributors list to be filled later\n",
    "            }\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        # Fetch authors and contributors for all papers\n",
    "        self._fetch_authors(cursor)\n",
    "        self._fetch_contributors(cursor)\n",
    "        \n",
    "        # Convert embeddings to numpy array\n",
    "        embeddings_array = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Create and build the FAISS index\n",
    "        dimension = embeddings_array.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings_array)\n",
    "        \n",
    "        print(f\"Built FAISS index with {self.index.ntotal} vectors of dimension {dimension}\")\n",
    "        return self.index\n",
    "    \n",
    "    def _fetch_authors(self, cursor):\n",
    "        \"\"\"\n",
    "        Fetch authors for all papers in paper_metadata\n",
    "        \n",
    "        Args:\n",
    "            cursor: MySQL database cursor\n",
    "        \"\"\"\n",
    "        if not self.paper_ids:\n",
    "            return\n",
    "            \n",
    "        # Format paper IDs for SQL IN clause\n",
    "        paper_ids_str = \", \".join([f\"'{pid}'\" for pid in self.paper_ids])\n",
    "        \n",
    "        # Query to get authors for all papers in one go\n",
    "        query = f\"\"\"\n",
    "        SELECT pc.paper_id, c.name \n",
    "        FROM paper_creators pc\n",
    "        JOIN creators c ON pc.creator_id = c.id\n",
    "        WHERE pc.paper_id IN ({paper_ids_str})\n",
    "        ORDER BY pc.paper_id, c.name\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        author_results = cursor.fetchall()\n",
    "        \n",
    "        # Group authors by paper_id\n",
    "        for paper_id, author_name in author_results:\n",
    "            if paper_id in self.paper_metadata:\n",
    "                self.paper_metadata[paper_id]['authors'].append(author_name)\n",
    "        \n",
    "        # Count papers with authors\n",
    "        papers_with_authors = sum(1 for pid in self.paper_ids if self.paper_metadata[pid]['authors'])\n",
    "        print(f\"Found authors for {papers_with_authors} out of {len(self.paper_ids)} papers\")\n",
    "        \n",
    "    def _fetch_contributors(self, cursor):\n",
    "        \"\"\"\n",
    "        Fetch contributors for all papers in paper_metadata\n",
    "        \n",
    "        Args:\n",
    "            cursor: MySQL database cursor\n",
    "        \"\"\"\n",
    "        if not self.paper_ids:\n",
    "            return\n",
    "            \n",
    "        # Format paper IDs for SQL IN clause\n",
    "        paper_ids_str = \", \".join([f\"'{pid}'\" for pid in self.paper_ids])\n",
    "        \n",
    "        # Query to get contributors for all papers in one go\n",
    "        query = f\"\"\"\n",
    "        SELECT pc.paper_id, c.name, pc.role\n",
    "        FROM paper_contributors pc\n",
    "        JOIN contributors c ON pc.contributor_id = c.id\n",
    "        WHERE pc.paper_id IN ({paper_ids_str})\n",
    "        ORDER BY pc.paper_id, pc.role, c.name\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor.execute(query)\n",
    "        contributor_results = cursor.fetchall()\n",
    "        \n",
    "        # Group contributors by paper_id\n",
    "        for paper_id, contributor_name, role in contributor_results:\n",
    "            if paper_id in self.paper_metadata:\n",
    "                self.paper_metadata[paper_id]['contributors'].append({\n",
    "                    'name': contributor_name,\n",
    "                    'role': role\n",
    "                })\n",
    "        \n",
    "        # Count papers with contributors\n",
    "        papers_with_contributors = sum(1 for pid in self.paper_ids if self.paper_metadata[pid]['contributors'])\n",
    "        print(f\"Found contributors for {papers_with_contributors} out of {len(self.paper_ids)} papers\")\n",
    "    \n",
    "    def search(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar papers using the provided query text\n",
    "        \n",
    "        Args:\n",
    "            query_text: The text query to search for\n",
    "            top_k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing search results with metadata and similarity scores\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call load_index first.\")\n",
    "            \n",
    "        # Convert query to embedding\n",
    "        query_embedding = np.array(get_embedding(query_text, self.model)).reshape(1, -1).astype('float32')\n",
    "        \n",
    "        # Search the index\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.paper_ids):\n",
    "                paper_id = self.paper_ids[idx]\n",
    "                metadata = self.paper_metadata[paper_id]\n",
    "                \n",
    "                # Calculate similarity score (convert distance to similarity)\n",
    "                similarity = 1 / (1 + distances[0][i])\n",
    "                \n",
    "                results.append({\n",
    "                    'id': paper_id,\n",
    "                    'title': metadata['title'],\n",
    "                    'abstract': metadata['abstract'],\n",
    "                    'authors': metadata['authors'],\n",
    "                    'contributors': metadata['contributors'],\n",
    "                    'similarity_score': similarity,\n",
    "                    'distance': float(distances[0][i])\n",
    "                })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51ee4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new search engine instance...\n",
      "Using title embeddings (iniLM-L6-v2)\n",
      "Loaded 41597 papers with embeddings\n",
      "Loaded 41597 papers with embeddings\n",
      "Found authors for 41597 out of 41597 papers\n",
      "Found authors for 41597 out of 41597 papers\n",
      "Found contributors for 41597 out of 41597 papers\n",
      "Built FAISS index with 41597 vectors of dimension 384\n",
      "\n",
      "Search engine ready for queries!\n",
      "Found contributors for 41597 out of 41597 papers\n",
      "Built FAISS index with 41597 vectors of dimension 384\n",
      "\n",
      "Search engine ready for queries!\n"
     ]
    }
   ],
   "source": [
    "# Global variable to store the loaded search engine instance\n",
    "search_engine_global = None\n",
    "\n",
    "# Import the print_formatted_results function\n",
    "from print_formatted_results import print_formatted_results\n",
    "\n",
    "def load_search_engine(model, use_title=True, use_abstract=False, limit=None, force_reload=False):\n",
    "    \"\"\"\n",
    "    Load or return the cached search engine instance\n",
    "    \n",
    "    Args:\n",
    "        model: The SentenceTransformer model to use\n",
    "        use_title: Whether to use title embeddings\n",
    "        use_abstract: Whether to use abstract embeddings\n",
    "        limit: Optional limit on number of papers to load\n",
    "        force_reload: Whether to force reload the index even if already loaded\n",
    "        \n",
    "    Returns:\n",
    "        ThesisSimilaritySearch instance with loaded index\n",
    "    \"\"\"\n",
    "    global search_engine_global\n",
    "    \n",
    "    # Check if we already have a loaded search engine and we're not forcing a reload\n",
    "    if search_engine_global is not None and not force_reload:\n",
    "        print(\"Using cached search engine instance...\")\n",
    "        return search_engine_global\n",
    "    \n",
    "    # Create a new search engine instance\n",
    "    print(\"Creating new search engine instance...\")\n",
    "    search_engine = ThesisSimilaritySearch(model=model, use_title=use_title, use_abstract=use_abstract)\n",
    "    \n",
    "    # Load the index\n",
    "    try:\n",
    "        search_engine.load_index(cursor, limit=limit)\n",
    "        # Cache the loaded search engine\n",
    "        search_engine_global = search_engine\n",
    "        return search_engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading search engine: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# Load the search engine (first time will load from database)\n",
    "try:\n",
    "    search_engine = load_search_engine(model=model, use_title=True, use_abstract=False)\n",
    "    print(\"\\nSearch engine ready for queries!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing search engine: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86f4bb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Reload with different parameters\\nsearch_engine = reload_search_engine(use_title=False, use_abstract=True, limit=1000)\\n\\n# Check that it worked\\nif search_engine:\\n    print(f\"Successfully reloaded search engine\")\\n    # Run a test search\\n    results = search_engine.search(\"machine learning\", top_k=2)\\n    print_formatted_results(results)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to reload the index with different parameters if needed\n",
    "\n",
    "def reload_search_engine(use_title=True, use_abstract=False, limit=None):\n",
    "    \"\"\"\n",
    "    Force reload the search engine with different parameters\n",
    "    \n",
    "    Args:\n",
    "        use_title: Whether to use title embeddings\n",
    "        use_abstract: Whether to use abstract embeddings\n",
    "        limit: Optional limit on number of papers to load\n",
    "    \"\"\"\n",
    "    global search_engine_global\n",
    "    print(\"Forcing reload of search engine...\")\n",
    "    \n",
    "    # Set the global variable to None to ensure we reload\n",
    "    search_engine_global = None\n",
    "    \n",
    "    # Call load_search_engine with force_reload=True\n",
    "    return load_search_engine(\n",
    "        model=model, \n",
    "        use_title=use_title, \n",
    "        use_abstract=use_abstract, \n",
    "        limit=limit, \n",
    "        force_reload=True\n",
    "    )\n",
    "\n",
    "# Example usage (commented out to prevent accidental execution)\n",
    "'''\n",
    "# Reload with different parameters\n",
    "search_engine = reload_search_engine(use_title=False, use_abstract=True, limit=1000)\n",
    "\n",
    "# Check that it worked\n",
    "if search_engine:\n",
    "    print(f\"Successfully reloaded search engine\")\n",
    "    # Run a test search\n",
    "    results = search_engine.search(\"machine learning\", top_k=2)\n",
    "    print_formatted_results(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3dec78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for displaying search results with customizable options\n",
    "\n",
    "def print_formatted_results(results, show_authors=True, show_contributors=True, show_abstract=True, show_metrics=True):\n",
    "    \"\"\"\n",
    "    Print formatted search results with customizable display options\n",
    "    \n",
    "    Args:\n",
    "        results: List of search results from ThesisSimilaritySearch.search()\n",
    "        show_authors: Whether to display author information\n",
    "        show_contributors: Whether to display contributor information\n",
    "        show_abstract: Whether to display the abstract\n",
    "        show_metrics: Whether to display similarity score and distance\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "        \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}: {result['title']}\")\n",
    "        \n",
    "        # Display metrics if requested\n",
    "        if show_metrics:\n",
    "            print(f\"  Similarity: {result['similarity_score']:.4f}\")\n",
    "            print(f\"  Distance: {result['distance']:.4f}\")\n",
    "        \n",
    "        # Display authors if requested\n",
    "        if show_authors and 'authors' in result:\n",
    "            if result['authors']:\n",
    "                authors_str = \", \".join(result['authors'])\n",
    "                print(f\"  Authors: {authors_str}\")\n",
    "            else:\n",
    "                print(f\"  Authors: No author information available\")\n",
    "        \n",
    "        # Display contributors if requested\n",
    "        if show_contributors and 'contributors' in result:\n",
    "            if result['contributors']:\n",
    "                # Group contributors by role\n",
    "                contributors_by_role = {}\n",
    "                for contributor in result['contributors']:\n",
    "                    role = contributor['role']\n",
    "                    if role not in contributors_by_role:\n",
    "                        contributors_by_role[role] = []\n",
    "                    contributors_by_role[role].append(contributor['name'])\n",
    "                \n",
    "                # Print contributors by role\n",
    "                print(f\"  Contributors:\")\n",
    "                for role, names in sorted(contributors_by_role.items()):\n",
    "                    contributors_str = \", \".join(names)\n",
    "                    print(f\"    {role}: {contributors_str}\")\n",
    "            else:\n",
    "                print(f\"  Contributors: No contributor information available\")\n",
    "        \n",
    "        # Display abstract if requested\n",
    "        if show_abstract and 'abstract' in result:\n",
    "            if result['abstract']:\n",
    "                # Truncate very long abstracts for display\n",
    "                abstract = result['abstract']\n",
    "                if len(abstract) > 300:\n",
    "                    abstract = abstract[:297] + '...'\n",
    "                print(f\"  Abstract: {abstract}\")\n",
    "            else:\n",
    "                print(f\"  Abstract: No abstract available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f75c1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'diabetes detection with machine learning'\n",
      "\n",
      "Result 1: Perbandingan algoritma Naive-Bayes, K-NN dan Decision Tree dalam pengklasifikasian data penyakit diabetes\n",
      "  Similarity: 0.5666\n",
      "  Distance: 0.7650\n",
      "  Authors: NICOLAS OWEN\n",
      "  Contributors:\n",
      "    Advisor 1: Alexander Setiawan\n",
      "    Advisor 2: Henry Novianus Palit, S.Kom., M.Kom., Ph.D.\n",
      "    Examination Committee 1: Agustinus Noertjahyana\n",
      "    Examination Committee 2: Rolly Intan\n",
      "  Abstract: Diabetes adalah kondisi kronis yang memiliki dampak serius pada kesehatan. Kondisi ini dapat menyebabkan kerusakan pada organ tubuh seperti mata, ginjal, saraf, serta memengaruhi jantung dan pembuluh darah. Akibatnya, risiko terkena stroke, serangan jantung, gangguan penglihatan, amputasi, dan ga...\n",
      "\n",
      "Result 2: Perancangan buku interaktif pencegahan diabetes melitus pada anak-anak\n",
      "  Similarity: 0.4966\n",
      "  Distance: 1.0136\n",
      "  Authors: JESSICA W WILIANTO\n",
      "  Contributors:\n",
      "    Advisor 1: Andrian Dektisa Hagijanto\n",
      "    Advisor 2: Jacky Cahyadi\n",
      "    Examination Committee 1: Ani Wijayanti Suhartono\n",
      "  Abstract: Salah satu penyakit yang dewasa ini merabah masyarakat terutama anak-anak di Surabaya yaitu Diabetes Melitus. Salah satu upaya yang dapat dilakukan untuk mengurangi resiko penyakit tersebut pada anak-anak adalah dengan memberikan edukasi tentang penyakit tersebut sejak dini. Perancangan buku inte...\n",
      "\n",
      "Result 3: Fasilitas penanggulangan dini dan perawatan penyakit diabetes melitus di Surabaya\n",
      "  Similarity: 0.4884\n",
      "  Distance: 1.0476\n",
      "  Authors: RIANTHY SULISTHIO\n",
      "  Contributors:\n",
      "    Advisor 1: Luciana Kristanto, S.T., M.T.\n",
      "    Advisor 2: Irwan Santoso\n",
      "    Advisor 3: Ir. Bisatya Widadya Maer, M.T.\n",
      "  Abstract: Proyek tugas akhir ini merupakan fasilitas penanganan penyakit diabetes melitus. Pada fasilitas ini memenuhi empat pilar penanganan penyakit diabetes, yaitu perancangan makan, pelatihan jasmani, edukasi dan obat-obatan. Berangkat dari empat pilar penanganan penyakit diabetes maka fasilitas ini me...\n",
      "\n",
      "Result 4: Perancangan dokumen interaktif tentang pencegahan diabetes melitus untuk remaja usia 15-17 tahun\n",
      "  Similarity: 0.4803\n",
      "  Distance: 1.0819\n",
      "  Authors: DHANU CAHAYA HERYOKO\n",
      "  Contributors:\n",
      "    Advisor 1: BRAMANTYO\n",
      "    Advisor 2: Ryan Pratama Sutanto, S.Sn., M.Med.Kom.\n",
      "    Examination Committee 1: Aristarchus Pranayana\n",
      "  Abstract: Perancangan ini bertujuan untuk memberikan informasi kepada remaja usia 15-17 tahun, bahwa dalam rentang usia mereka pun, hari ini diabetes melitus sudah bisa menyerang mereka. Dengan dibuatnya perancangan ini, secara tidak langsung juga memberikan fakta mengenai metabolisme tubuh dan gaya hidup ...\n",
      "\n",
      "Result 5: Penerapan metode convolutional neural network dalam identifikasi penyakit diabetes retinopathy\n",
      "  Similarity: 0.4705\n",
      "  Distance: 1.1254\n",
      "  Authors: DODDY SAPUTRA WIBISONO\n",
      "  Contributors:\n",
      "    Advisor 1: Silvia Rostianingsih\n",
      "    Advisor 2: Kartika Gunadi\n",
      "    Examination Committee 1: Andreas Handojo\n",
      "    Examination Committee 2: Stephanus A. Ananda, S.T., M.Sc. Ph.D.\n",
      "  Abstract: Kesehatan merupakan aspek yang sangat penting bagi kehidupan manusia sehari-hari. Namun, kesadaran akan pentingnya menjaga kesehatan tubuh masih rendah di kalangan masyarakat. Salah satu penyakit yang dapat menyerang tubuh manusia adalah Diabetes Retinopathy, sebuah komplikasi dari diabetes yang ...\n",
      "\n",
      "Result 6: Evaluasi algoritma machine learning dalam menentukan diagnosis dan obat berdasarkan gejala pasien\n",
      "  Similarity: 0.4533\n",
      "  Distance: 1.2061\n",
      "  Authors: VINCENTIUS ANDY\n",
      "  Contributors:\n",
      "    Advisor 1: Stephanus A. Ananda, S.T., M.Sc. Ph.D.\n",
      "    Examination Committee 1: Gregorius Satiabudhi\n",
      "  Abstract: Penelitian ini bertujuan untuk menentukan algoritma machine learning terbaik dalam membantu dokter memberikan diagnosis dan rekomendasi obat berdasarkan gejala yang dialami pasien. Mengingat pentingnya diagnosis yang akurat dalam dunia medis, kesalahan diagnosis atau misdiagnosis dapat berdampak ...\n",
      "\n",
      "Result 7: Perbandingan metode machine learning support vector machine, K-nearest neighbor dan random forest dalam prediksi penyakit stroke\n",
      "  Similarity: 0.4493\n",
      "  Distance: 1.2257\n",
      "  Authors: BRYANT SEPTIYAN BUDIHARJO\n",
      "  Contributors:\n",
      "    Advisor 1: Alexander Setiawan\n",
      "    Advisor 2: Henry Novianus Palit, S.Kom., M.Kom., Ph.D.\n",
      "    Examination Committee 1: Gregorius Satiabudhi, Rolly Intan\n",
      "  Abstract: Stroke merupakan salah satu penyakit yang dianggap sangat berbahaya karena penyakit ini sendiri adalah penyebab disabilitas nomor satu di dunia. Penyakit stroke terjadi ketika suplai darah ke bagian otak berkurang karena penyumbatan atau ketika pembuluh darah di otak pecah. Serangan stroke dapat ...\n",
      "\n",
      "Result 8: Perancangan komik strip sebagai media pendukung pencegahan diabetes di usia produktif 20-30 tahun\n",
      "  Similarity: 0.4440\n",
      "  Distance: 1.2521\n",
      "  Authors: YUNANTO WAHONO\n",
      "  Contributors:\n",
      "    Advisor 2: Rika Febriani, S.Sn., M.A.\n",
      "    Examination Committee 1: Bing Bedjo Tanudjaja\n",
      "  Abstract: Salah satu dari 5 besar penyakit penyebab kematian adalah diabetes. Diabetes atau yang dikenal dengan Kencing Manis dijuluki dengan Silent Killer dikarenakan penyakit ini dapat menyebabkan kematian secara bertahap. Dilematisnya, mayoritas penderita justru tidak menyadari gejala penyakit ini. Mere...\n",
      "\n",
      "Result 9: Perencanaan dan pembuatan alat test darah untuk mengukur kadar kolesterol dan glucose dalam darah dengan menggunakan IBM PC\n",
      "  Similarity: 0.4376\n",
      "  Distance: 1.2850\n",
      "  Authors: LAUW TIONG TIK\n",
      "  Contributors:\n",
      "    Advisor 1: HERLIANTO TENGGARA\n",
      "    Advisor 2: Heri Saptono Warpindyasmoro\n",
      "    Examination Committee 1: MOCHAMAD HEROE\n",
      "  Abstract: Penyakit jantung dan diabetes semakin banyak diderita orang saat ini, hal itu disebabkan banyaknya mengkomsumsi makanan dengan kadar lemak dan gula yang tinggi, sehingga kadar lemak dan gula dalam darah semakin tinggi. Karena kenaikan kadar lemak dalam darah ini menyebabkan terjadinya penyumbatan...\n",
      "\n",
      "Result 10: Analisis sentimen mahasiswa di Surabaya terhadap pelayanan vaksinasi covid-19 menggunakan beberapa classifier\n",
      "  Similarity: 0.4370\n",
      "  Distance: 1.2885\n",
      "  Authors: MELIANA KUSUMA PANGKASIDHI\n",
      "  Contributors:\n",
      "    Advisor 1: Henry Novianus Palit, S.Kom., M.Kom., Ph.D.\n",
      "    Advisor 2: Andre Gunawan\n",
      "    Examination Committee 1: Leo Willyanto Santoso\n",
      "    Examination Committee 2: Alvin Nathaniel Tjondrowiguno\n",
      "  Abstract: Indonesia merupakan salah satu negara yang sedang berjuang menangani pandemi virus COVID-19 pada saat ini. Untuk menangani pandemi ini, para ahli berlomba untuk memproduksi vaksin COVID-19. Pemerintah sekarang sedang berjuang mengajak masyarakat untuk melakukan vaksinasi, salah satunya adalah mem...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Search by author and topic\\nfiltered_results = search_by_people_and_topic(\\n    search_engine_global,\\n    \"artificial intelligence\", \\n    author_name=\"Johnson\",  # Replace with a name in your database\\n    top_k=3\\n)\\nprint_formatted_results(filtered_results)\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick search cell - run this for instant searches using the cached engine\n",
    "\n",
    "def quick_search(query, top_k=5, show_abstract=True):\n",
    "    \"\"\"\n",
    "    Perform a quick search using the cached search engine\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        top_k: Number of results to return\n",
    "        show_abstract: Whether to display abstracts\n",
    "    \"\"\"\n",
    "    global search_engine_global\n",
    "    \n",
    "    # Check if we have a cached search engine\n",
    "    if search_engine_global is None:\n",
    "        print(\"No cached search engine found. Loading index first...\")\n",
    "        load_search_engine(model=model)\n",
    "        if search_engine_global is None:\n",
    "            print(\"Failed to load search engine.\")\n",
    "            return\n",
    "    \n",
    "    # Perform the search\n",
    "    print(f\"Searching for: '{query}'\")\n",
    "    results = search_engine_global.search(query, top_k=top_k)\n",
    "    \n",
    "    # Display results\n",
    "    print_formatted_results(results, show_abstract=show_abstract)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Examples - uncomment and run any of these searches\n",
    "\n",
    "# Basic search\n",
    "results = quick_search(\"diabetes detection with machine learning\", top_k=10)\n",
    "\n",
    "# # Search without abstracts (more compact output)\n",
    "# results = quick_search(\"natural language processing\", top_k=5, show_abstract=False)\n",
    "\n",
    "'''\n",
    "# Search by author and topic\n",
    "filtered_results = search_by_people_and_topic(\n",
    "    search_engine_global,\n",
    "    \"artificial intelligence\", \n",
    "    author_name=\"Johnson\",  # Replace with a name in your database\n",
    "    top_k=3\n",
    ")\n",
    "print_formatted_results(filtered_results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe232f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using title embeddings (iniLM-L6-v2)\n",
      "Loaded 41597 papers with embeddings\n",
      "Loaded 41597 papers with embeddings\n",
      "Found authors for 41597 out of 41597 papers\n",
      "Found authors for 41597 out of 41597 papers\n",
      "Found contributors for 41597 out of 41597 papers\n",
      "Built FAISS index with 41597 vectors of dimension 384\n",
      "\n",
      "Sample search results:\n",
      "\n",
      "Query: 'neural networks for image classification'\n",
      "\n",
      "Result 1: Improving backpropagation training time and its generalization using pruning\n",
      "  Similarity: 0.4673\n",
      "  Authors: DANIEL BUDIONO\n",
      "  Contributors:\n",
      "    Advisor 1: LILIANA\n",
      "    Examination Committee 1: Gregorius Satiabudhi\n",
      "  Abstract: Beberapa tahun terakhir, banyak algoritma jaringan syaraf tiruan uang dikembangkan untuk klasifikasi pola. Salah satu algoritma yang populer adalah backpropagation. Akan tetapi, menentukan besarnya suatu jaringan backpropagation adalah suatu masalah yang sangat sulit. Jumlah hidden unit yang terlalu banyak akan menyebabkan jaringan terlalu menghafal data training dan kurang generalisasi. Sedangkan jumlah hidden unit yang terlalu sedikit, jaringan yang dihasilkan kurang belajar data yang ada. Pendekatan yang digunakan untuk mendapatkan jumlah hidden unit yang optimal adalah dengan memulai dengan jumlah yang berlebih kemudian menghilangkan hidden unit yang berlebih. Langkah kedua adalah menghilangkan input atribut yang berlebih. Langkah terakhir adalah menghilangkan weight yang berlebih. Aplikasi dibuat dengan C# dengan Microsoft Visual Studio 2005 sebagai IDE-nya. Hasil pengujian dengan beberapa data set menunjukan akurasi setelah proses pruning sama atau lebih baik. Akan tetapi, pada beberapa data set akurasai yang dihasilkan lebih rendah, tetapi masih dalam nilai yang wajar.\n",
      "  Distance: 1.1400327682495117\n",
      "\n",
      "Result 2: Studi beban harian (KWatt) pada MDP gedung P U.K. Petra dengan Artificial Neural Network Backpropagation\n",
      "  Similarity: 0.4631\n",
      "  Authors: ANDY IRAWAN\n",
      "  Contributors:\n",
      "    Advisor 1: Limboto Limantara\n",
      "    Examination Committee 1: Emmy Hosea\n",
      "  Abstract: Pada dunia industri terutama pada penggunaan bagian energi listrik sering kita dihadapkan dengan perhitungan-perhitungan data yang berjumlah banyak, terutama untuk Sistem Tenaga Listrik, dimana kita dituntut untuk memperhitungkan segala keperluan tentang energi listrik dengan se-efisien mungkin, maka diperlukan perhitungan-perhitungan terhadap pemakaian beban harian pada suatu industri. Studi dan pengukuran beban harian (kWatt) pada gedung P U.K. Petra ini bertujuan untuk memberikan gambaran tentang seberapa besar pemakaian beban listrik pada setiap harinya . Beban listrik ini akan diukur pada Circuit Breaker pusat pada panel M.D.P.(Main Distribution Panel) pada gedung P U.K. Petra yang merupakan pusat arus listrik yang mengalir ke seluruh gedung. Sebuah alat bantu yaitu Fluke 41B Power Harmonics Analyzer bertugas untuk mencatat beban (kWatt) harian pada tiap harinya secara nyata (real time), untuk kemudian diproses dengan program yaitu ANN Backpropagation dengan bantuan software Mathlab 6.5. dengan sistem operasi yang berbasis Microsoft Windows. Kesimpulan yang didapat, pengukuran data harian yang berjumlah besar ini sangatlah tepat bila diproses dan dianalisa dengan ANN Backpropagation karena hasil- hasil yang didapat sangat bagus dan mempunyai tingkat kesalahan yang cukup kecil dan dapat diabaikan. Hasil- hasil analisa pada grafik-grafik mapping yang didapat dapat dijadikan acuan atau pedoman untuk dijadikan referensi untuk perhitungan-perhitungan lain yang membutuhkan informasi tentang beban harian ini. Untuk mendapatkan hasil pengukuran yang lebih akurat lagi maka perlu diambil sampel data yang lebih banyak lagi agar perhitungan dan analisa lebih sempurna lagi. Bila hasil yang didapat sudah baik, maka nantinya program ini dapat digunakan untuk perhitungan-perhitungan di masa mendatang.\n",
      "  Distance: 1.1593763828277588\n",
      "\n",
      "Result 3: Prediksi jadwal pembayaran piutang kredit oleh nasabah Bank Perkreditan Rakyat dengan Algoritma Feed Forward Neural Network dan LSTM\n",
      "  Similarity: 0.4631\n",
      "  Authors: WILLIAM\n",
      "  Contributors:\n",
      "    Advisor 1: Ir. Djoni Haryadi Setiabudi, M.Eng.\n",
      "    Examination Committee 1: Kartika Gunadi\n",
      "    Examination Committee 2: Henry Novianus Palit, S.Kom., M.Kom., Ph.D.\n",
      "  Abstract: Bank Perkreditan Rakyat (BPR) sering menghadapi masalah dalam mengelola piutang kredit, terutama dalam hal pemulihan piutang yang macet atau bermasalah. Masalah kredit macet, atau Non-Performing Loan (NPL), menjadi perhatian serius karena rasio NPL yang tinggi dapat memburuk kondisi keuangan BPR dan mempengaruhi kemampuan mereka dalam memberikan layanan keuangan kepada nasabah lainnya. Pihak Bank Perkreditan Rakyat (BPR) telah mengimplementasikan beberapa strategi untuk mengatasi permasalahan tunggakan piutang. Langkah-langkah ini melibatkan penyaringan nasabah melalui kredit scoring sebagai langkah awal dalam memberikan kredit, menggunakan agen penagih hutang, dan merujuk pada ketentuan POJK 1 Tahun 2024 Bab 4 Pasal 29 Ayat 1-2, yang memberikan kewenangan kepada BPR untuk melakukan restrukturisasi kredit kepada debitur. Meskipun telah diimplementasikan berbagai strategi, BPR sering menghadapi masalah dalam mengelola piutang kredit, terutama dalam hal pemulihan piutang yang macet atau bermasalah. Masalah kredit macet, atau Non-Performing Loan (NPL), menjadi perhatian serius karena rasio NPL yang tinggi dapat memperburuk kondisi keuangan BPR dan mempengaruhi kemampuan mereka dalam memberikan layanan keuangan kepada nasabah lainnya. Pada tahun 2023, NPL untuk BPR mencapai angka 10,13%, jauh di atas rata-rata yang wajar, yang seharusnya di bawah 5%.Sudah banyak penelitian lain yang menghadapi masalah serupa, namun banyak di antaranya masih belum berhasil mencapai hasil yang memadai. Oleh karena itu penelitian ini bertujuan untuk memprediksi jadwal pembayaran piutang kredit oleh nasabah BPR menggunakan algoritma Feed Forward Neural Network (FFNN) dan Long Short-Term Memory (LSTM). Dengan memanfaatkan teknik machine learning, penelitian ini diharapkan dapat memberikan solusi efektif dalam mengelola arus kas dan meminimalkan risiko kredit macet. Metode yang digunakan mencakup studi literatur mengenai BPR, piutang, dan algoritma machine learning. Data piutang kredit diperoleh dari Bank Perkreditan Rakyat Intan Kita, kemudian dilakukan preprocessing untuk membersihkan data dan menyiapkannya untuk digunakan dalam model prediktif. Dataset dibagi menjadi data training dan testing dengan perbandingan 80:20 menggunakan teknik time split based. Model FFNN, LSTM, dan kombinasi FFNN-LSTM dibangun dan diuji performanya dengan metrik Mean Squared Error (MSE). Hasil penelitian menunjukkan bahwa model FFNN memberikan performa terbaik dalam memprediksi jadwal pembayaran piutang nasabah. Model ini mampu menangani data time series dengan cukup baik dengan nilai akurasi sebesar 73.03% tanpa adanya toleransi selisih keterlambatan, sehingga dapat disimpulkan model dapat menghasilkan prediksi yang akurat dan membantu BPR dalam mengelola arus kas mereka lebih efektif.\n",
      "  Distance: 1.1595243215560913\n",
      "\n",
      "To search for papers similar to your query:\n",
      "results = search_engine.search('your query here', top_k=5)\n",
      "Found contributors for 41597 out of 41597 papers\n",
      "Built FAISS index with 41597 vectors of dimension 384\n",
      "\n",
      "Sample search results:\n",
      "\n",
      "Query: 'neural networks for image classification'\n",
      "\n",
      "Result 1: Improving backpropagation training time and its generalization using pruning\n",
      "  Similarity: 0.4673\n",
      "  Authors: DANIEL BUDIONO\n",
      "  Contributors:\n",
      "    Advisor 1: LILIANA\n",
      "    Examination Committee 1: Gregorius Satiabudhi\n",
      "  Abstract: Beberapa tahun terakhir, banyak algoritma jaringan syaraf tiruan uang dikembangkan untuk klasifikasi pola. Salah satu algoritma yang populer adalah backpropagation. Akan tetapi, menentukan besarnya suatu jaringan backpropagation adalah suatu masalah yang sangat sulit. Jumlah hidden unit yang terlalu banyak akan menyebabkan jaringan terlalu menghafal data training dan kurang generalisasi. Sedangkan jumlah hidden unit yang terlalu sedikit, jaringan yang dihasilkan kurang belajar data yang ada. Pendekatan yang digunakan untuk mendapatkan jumlah hidden unit yang optimal adalah dengan memulai dengan jumlah yang berlebih kemudian menghilangkan hidden unit yang berlebih. Langkah kedua adalah menghilangkan input atribut yang berlebih. Langkah terakhir adalah menghilangkan weight yang berlebih. Aplikasi dibuat dengan C# dengan Microsoft Visual Studio 2005 sebagai IDE-nya. Hasil pengujian dengan beberapa data set menunjukan akurasi setelah proses pruning sama atau lebih baik. Akan tetapi, pada beberapa data set akurasai yang dihasilkan lebih rendah, tetapi masih dalam nilai yang wajar.\n",
      "  Distance: 1.1400327682495117\n",
      "\n",
      "Result 2: Studi beban harian (KWatt) pada MDP gedung P U.K. Petra dengan Artificial Neural Network Backpropagation\n",
      "  Similarity: 0.4631\n",
      "  Authors: ANDY IRAWAN\n",
      "  Contributors:\n",
      "    Advisor 1: Limboto Limantara\n",
      "    Examination Committee 1: Emmy Hosea\n",
      "  Abstract: Pada dunia industri terutama pada penggunaan bagian energi listrik sering kita dihadapkan dengan perhitungan-perhitungan data yang berjumlah banyak, terutama untuk Sistem Tenaga Listrik, dimana kita dituntut untuk memperhitungkan segala keperluan tentang energi listrik dengan se-efisien mungkin, maka diperlukan perhitungan-perhitungan terhadap pemakaian beban harian pada suatu industri. Studi dan pengukuran beban harian (kWatt) pada gedung P U.K. Petra ini bertujuan untuk memberikan gambaran tentang seberapa besar pemakaian beban listrik pada setiap harinya . Beban listrik ini akan diukur pada Circuit Breaker pusat pada panel M.D.P.(Main Distribution Panel) pada gedung P U.K. Petra yang merupakan pusat arus listrik yang mengalir ke seluruh gedung. Sebuah alat bantu yaitu Fluke 41B Power Harmonics Analyzer bertugas untuk mencatat beban (kWatt) harian pada tiap harinya secara nyata (real time), untuk kemudian diproses dengan program yaitu ANN Backpropagation dengan bantuan software Mathlab 6.5. dengan sistem operasi yang berbasis Microsoft Windows. Kesimpulan yang didapat, pengukuran data harian yang berjumlah besar ini sangatlah tepat bila diproses dan dianalisa dengan ANN Backpropagation karena hasil- hasil yang didapat sangat bagus dan mempunyai tingkat kesalahan yang cukup kecil dan dapat diabaikan. Hasil- hasil analisa pada grafik-grafik mapping yang didapat dapat dijadikan acuan atau pedoman untuk dijadikan referensi untuk perhitungan-perhitungan lain yang membutuhkan informasi tentang beban harian ini. Untuk mendapatkan hasil pengukuran yang lebih akurat lagi maka perlu diambil sampel data yang lebih banyak lagi agar perhitungan dan analisa lebih sempurna lagi. Bila hasil yang didapat sudah baik, maka nantinya program ini dapat digunakan untuk perhitungan-perhitungan di masa mendatang.\n",
      "  Distance: 1.1593763828277588\n",
      "\n",
      "Result 3: Prediksi jadwal pembayaran piutang kredit oleh nasabah Bank Perkreditan Rakyat dengan Algoritma Feed Forward Neural Network dan LSTM\n",
      "  Similarity: 0.4631\n",
      "  Authors: WILLIAM\n",
      "  Contributors:\n",
      "    Advisor 1: Ir. Djoni Haryadi Setiabudi, M.Eng.\n",
      "    Examination Committee 1: Kartika Gunadi\n",
      "    Examination Committee 2: Henry Novianus Palit, S.Kom., M.Kom., Ph.D.\n",
      "  Abstract: Bank Perkreditan Rakyat (BPR) sering menghadapi masalah dalam mengelola piutang kredit, terutama dalam hal pemulihan piutang yang macet atau bermasalah. Masalah kredit macet, atau Non-Performing Loan (NPL), menjadi perhatian serius karena rasio NPL yang tinggi dapat memburuk kondisi keuangan BPR dan mempengaruhi kemampuan mereka dalam memberikan layanan keuangan kepada nasabah lainnya. Pihak Bank Perkreditan Rakyat (BPR) telah mengimplementasikan beberapa strategi untuk mengatasi permasalahan tunggakan piutang. Langkah-langkah ini melibatkan penyaringan nasabah melalui kredit scoring sebagai langkah awal dalam memberikan kredit, menggunakan agen penagih hutang, dan merujuk pada ketentuan POJK 1 Tahun 2024 Bab 4 Pasal 29 Ayat 1-2, yang memberikan kewenangan kepada BPR untuk melakukan restrukturisasi kredit kepada debitur. Meskipun telah diimplementasikan berbagai strategi, BPR sering menghadapi masalah dalam mengelola piutang kredit, terutama dalam hal pemulihan piutang yang macet atau bermasalah. Masalah kredit macet, atau Non-Performing Loan (NPL), menjadi perhatian serius karena rasio NPL yang tinggi dapat memperburuk kondisi keuangan BPR dan mempengaruhi kemampuan mereka dalam memberikan layanan keuangan kepada nasabah lainnya. Pada tahun 2023, NPL untuk BPR mencapai angka 10,13%, jauh di atas rata-rata yang wajar, yang seharusnya di bawah 5%.Sudah banyak penelitian lain yang menghadapi masalah serupa, namun banyak di antaranya masih belum berhasil mencapai hasil yang memadai. Oleh karena itu penelitian ini bertujuan untuk memprediksi jadwal pembayaran piutang kredit oleh nasabah BPR menggunakan algoritma Feed Forward Neural Network (FFNN) dan Long Short-Term Memory (LSTM). Dengan memanfaatkan teknik machine learning, penelitian ini diharapkan dapat memberikan solusi efektif dalam mengelola arus kas dan meminimalkan risiko kredit macet. Metode yang digunakan mencakup studi literatur mengenai BPR, piutang, dan algoritma machine learning. Data piutang kredit diperoleh dari Bank Perkreditan Rakyat Intan Kita, kemudian dilakukan preprocessing untuk membersihkan data dan menyiapkannya untuk digunakan dalam model prediktif. Dataset dibagi menjadi data training dan testing dengan perbandingan 80:20 menggunakan teknik time split based. Model FFNN, LSTM, dan kombinasi FFNN-LSTM dibangun dan diuji performanya dengan metrik Mean Squared Error (MSE). Hasil penelitian menunjukkan bahwa model FFNN memberikan performa terbaik dalam memprediksi jadwal pembayaran piutang nasabah. Model ini mampu menangani data time series dengan cukup baik dengan nilai akurasi sebesar 73.03% tanpa adanya toleransi selisih keterlambatan, sehingga dapat disimpulkan model dapat menghasilkan prediksi yang akurat dan membantu BPR dalam mengelola arus kas mereka lebih efektif.\n",
      "  Distance: 1.1595243215560913\n",
      "\n",
      "To search for papers similar to your query:\n",
      "results = search_engine.search('your query here', top_k=5)\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of using the ThesisSimilaritySearch class with caching\n",
    "\n",
    "# Get the cached search engine or load if not already loaded\n",
    "try:\n",
    "    # Use the cached search engine\n",
    "    search_engine = load_search_engine(model=model, use_title=True, use_abstract=False)\n",
    "    \n",
    "    if search_engine is not None:\n",
    "        # Example queries\n",
    "        example_queries = [\n",
    "            \"natural language processing in healthcare\",\n",
    "            # \"machine learning for computer vision\",\n",
    "            # \"data mining techniques\",\n",
    "            # \"information retrieval systems\",\n",
    "            # \"neural networks for image classification\"\n",
    "        ]\n",
    "        \n",
    "        # Run a sample search\n",
    "        print(\"\\nSample search results:\")\n",
    "        for query in example_queries[:2]:  # Just show results for first two queries\n",
    "            print(f\"\\nQuery: '{query}'\")\n",
    "            results = search_engine.search(query, top_k=3)\n",
    "            \n",
    "            for i, result in enumerate(results):\n",
    "                print(f\"\\nResult {i+1}: {result['title']}\")\n",
    "                print(f\"  Similarity: {result['similarity_score']:.4f}\")\n",
    "                \n",
    "                # Display authors\n",
    "                if result['authors']:\n",
    "                    authors_str = \", \".join(result['authors'])\n",
    "                    print(f\"  Authors: {authors_str}\")\n",
    "                else:\n",
    "                    print(f\"  Authors: No author information available\")\n",
    "                \n",
    "                # Display contributors\n",
    "                if result['contributors']:\n",
    "                    # Group contributors by role\n",
    "                    contributors_by_role = {}\n",
    "                    for contributor in result['contributors']:\n",
    "                        role = contributor['role']\n",
    "                        if role not in contributors_by_role:\n",
    "                            contributors_by_role[role] = []\n",
    "                        contributors_by_role[role].append(contributor['name'])\n",
    "                    \n",
    "                    # Print contributors by role\n",
    "                    print(f\"  Contributors:\")\n",
    "                    for role, names in contributors_by_role.items():\n",
    "                        contributors_str = \", \".join(names)\n",
    "                        print(f\"    {role}: {contributors_str}\")\n",
    "                else:\n",
    "                    print(f\"  Contributors: No contributor information available\")\n",
    "                    \n",
    "                print(f\"  Abstract: {result['abstract']}\")\n",
    "                print(f\"  Distance: {result['distance']}\")\n",
    "        \n",
    "        print(\"\\nTo search for papers similar to your query:\")\n",
    "        print(\"results = search_engine.search('your query here', top_k=5)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error using the search engine: {e}\")\n",
    "    print(\"Make sure you have papers with embeddings in your database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-thesis-similarity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
